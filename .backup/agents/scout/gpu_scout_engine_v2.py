"""
Next-Generation GPU-Accelerated Scout Intelligence Engine
Production-Ready AI-First Architecture with Specialized Models

Features:
- BERT-based news classification (AI-first approach)
- DeBERTa content quality assessment
- RoBERTa bias detection
- Local LLaVA for visual content analysis
- Structured output generation
- Model training and fine-tuning capabilities
- Production-ready warning suppression
"""

import os
import warnings
from datetime import UTC, datetime
from typing import Any

import torch
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TrainingArguments,
)
from transformers import logging as transformers_logging

from common.observability import get_logger

# Handle problematic imports with fallbacks
try:
    from transformers import pipeline
    PIPELINE_AVAILABLE = True
except ImportError:
    PIPELINE_AVAILABLE = False
    def pipeline(*args, **kwargs):
        raise ImportError("Pipeline not available due to transformers/torchvision compatibility issue")

try:
    from transformers import LlavaNextForConditionalGeneration, LlavaNextProcessor
    LLAVA_AVAILABLE = True
except ImportError:
    LLAVA_AVAILABLE = False
    LlavaNextProcessor = None
    LlavaNextForConditionalGeneration = None

try:
    from transformers import Trainer
    TRAINER_AVAILABLE = True
except ImportError:
    TRAINER_AVAILABLE = False
    Trainer = None

try:
    from transformers import PreTrainedModel
    PRETRAINED_MODEL_AVAILABLE = True
except ImportError:
    PRETRAINED_MODEL_AVAILABLE = False
    PreTrainedModel = None
from torch.utils.data import Dataset

# Suppress known deprecation warnings for production
warnings.filterwarnings("ignore", category=FutureWarning, module="torch.nn.modules.module")
warnings.filterwarnings("ignore", category=UserWarning, module="transformers.pipelines.text_classification")
warnings.filterwarnings("ignore", category=UserWarning, module="transformers.models")
warnings.filterwarnings("ignore", category=FutureWarning, module="transformers")

# Set transformers logging to ERROR to reduce noise
transformers_logging.set_verbosity_error()

# Production GPU Manager integration
try:
    from agents.common.gpu_manager import (
        get_gpu_manager,
        release_agent_gpu,
        request_agent_gpu,
    )
    PRODUCTION_GPU_AVAILABLE = True
except ImportError:
    PRODUCTION_GPU_AVAILABLE = False

logger = get_logger(__name__)

class OptimizedNewsDataset(Dataset):
    """Custom dataset for fine-tuning news classification models"""

    def __init__(self, texts: list[str], labels: list[int], tokenizer, max_length: int = 512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class NextGenGPUScoutEngine:
    """
    Next-Generation GPU-Accelerated Scout Intelligence Engine
    
    Features:
    - AI-first approach with specialized models
    - BERT-based news classification
    - DeBERTa content quality assessment
    - RoBERTa sentiment analysis (high-quality)
    - RoBERTa bias detection
    - Local LLaVA for visual content analysis
    - Model training and fine-tuning
    - Production-ready warning suppression
    """

    def __init__(self, enable_training: bool = False, device: str | None = None):
        """
        Initialize Next-Gen GPU Scout Engine with specialized models
        
        Args:
            enable_training: Enable model training capabilities
            device: GPU device ('cuda' or 'cpu')
        """
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.enable_training = enable_training
        self.models = {}
        self.pipelines = {}
        self.processors = {}

        # Enhanced model configurations for specialized tasks
        self.model_configs = {
            "news_classifier": {
                "model_name": "google-bert/bert-base-uncased",
                "task": "news_classification",
                "revision": "main",  # Specify revision for production stability
                "num_labels": 2,  # Binary: news/not-news
                "batch_size": 32
            },
            "quality_assessor": {
                "model_name": "google-bert/bert-base-uncased",
                "task": "quality_assessment",
                "revision": "main",  # Specify revision for production stability
                "num_labels": 3,  # Low/Medium/High quality
                "batch_size": 16
            },
            "sentiment_analyzer": {
                "model_name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                "task": "sentiment_analysis",
                "revision": "main",  # Specify revision for production stability
                "batch_size": 24
            },
            "bias_detector": {
                "model_name": "martin-ha/toxic-comment-model",
                "task": "bias_detection",
                "revision": "main",  # Specify revision for production stability
                "num_labels": 2,  # Binary: biased/not-biased
                "batch_size": 16
            },
            "visual_analyzer": {
                "model_name": "llava-hf/llava-onevision-qwen2-0.5b-ov-hf",
                "task": "visual_analysis",
                "revision": "main",  # Specify revision for production stability
                "max_new_tokens": 200,
                "batch_size": 1
            }
        }

        # Initialize tokenizer storage
        self.tokenizers = {}

        # Initialize GPU allocation for scout agent
        self.gpu_allocated = False
        self.gpu_device = None
        self.gpu_memory_gb = 4.0  # Scout needs ~4GB for models

        if PRODUCTION_GPU_AVAILABLE and self.device.startswith("cuda"):
            try:
                # Request GPU allocation through production manager
                allocation = request_agent_gpu('scout', self.gpu_memory_gb)
                if isinstance(allocation, dict) and allocation.get('status') == 'allocated':
                    self.gpu_allocated = True
                    self.gpu_device = allocation.get('gpu_device', 0)
                    self.gpu_memory_gb = allocation.get('allocated_memory_gb', self.gpu_memory_gb)
                    logger.info(f"‚úÖ Scout GPU allocated: {self.gpu_memory_gb}GB on device {self.gpu_device}")
                else:
                    logger.warning(f"‚ö†Ô∏è Production GPU manager allocation failed: {allocation}")
                    # Fall back to direct GPU access instead of CPU
                    if torch.cuda.is_available():
                        self.gpu_allocated = True
                        self.gpu_device = 0  # Use first available GPU
                        logger.info(f"üìã Production GPU manager failed, using direct GPU access on device {self.gpu_device}")
                    else:
                        logger.warning("‚ö†Ô∏è No CUDA devices available, falling back to CPU")
                        self.device = 'cpu'
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Production GPU manager error: {e}")
                # Fall back to direct GPU access instead of CPU
                if torch.cuda.is_available():
                    self.gpu_allocated = True
                    self.gpu_device = 0  # Use first available GPU
                    logger.info(f"üìã Production GPU manager error, using direct GPU access on device {self.gpu_device}")
                else:
                    logger.warning("‚ö†Ô∏è No CUDA devices available, falling back to CPU")
                    self.device = 'cpu'
        else:
            logger.info("üìã Production GPU manager not available, using direct GPU access")

        # Initialize specialized models
        logger.info(f"üöÄ Initializing Next-Gen GPU Scout Engine on {self.device}")
        self._initialize_specialized_models()

    def _initialize_specialized_models(self):
        """Initialize all specialized AI models for different tasks"""

        try:
            # 1. News Classification Model (BERT-based) - Production Ready
            logger.info("üî• Loading News Classification Model (BERT)...")
            news_config = self.model_configs["news_classifier"]

            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                # Use GPU orchestrator for model management
                gpu_mgr = get_gpu_manager()
                model_key = f"scout_news_classifier"

                # Check if model already exists in registry
                existing_pipeline = gpu_mgr.get(model_key)
                if existing_pipeline:
                    self.pipelines["news_classifier"] = existing_pipeline
                    self.models["news_classifier"] = True
                    self.tokenizers["news_classifier"] = True  # Mark as available
                    logger.info("‚úÖ News Classification Model loaded from GPU orchestrator registry")
                else:
                    try:
                        from agents.common.model_loader import load_transformers_model
                        model, tokenizer = load_transformers_model(
                            news_config["model_name"],
                            agent='scout',
                            cache_dir=None,
                            model_class=AutoModelForSequenceClassification,
                            tokenizer_class=AutoTokenizer,
                        )
                        try:
                            model = model.to(self.device)
                        except Exception:
                            pass
                        self.models["news_classifier"] = model
                        self.tokenizers["news_classifier"] = tokenizer
                    except Exception:
                        self.models["news_classifier"] = AutoModelForSequenceClassification.from_pretrained(
                            news_config["model_name"],
                            num_labels=news_config["num_labels"],
                            dtype=torch.float16 if self.device.startswith("cuda") else torch.float32,
                            trust_remote_code=True,
                            use_auth_token=False
                        ).to(self.device)

                        self.tokenizers["news_classifier"] = AutoTokenizer.from_pretrained(
                            news_config["model_name"],
                            trust_remote_code=True,
                            use_fast=True  # Use fast tokenizer to avoid deprecation
                        )

                    self.pipelines["news_classifier"] = pipeline(
                        "text-classification",
                        model=self.models["news_classifier"],
                        tokenizer=self.tokenizers["news_classifier"],
                        revision=news_config.get("revision", "main"),  # Use revision for production stability
                        device=0 if self.device.startswith("cuda") else -1,
                        top_k=None,  # Updated API - replaces return_all_scores=True
                        batch_size=1,
                        truncation=True,
                        max_length=512
                    )

                    # Register with GPU orchestrator
                    gpu_mgr.register_model(model_key, self.pipelines["news_classifier"])
                    gpu_mgr.register_model(f"{model_key}_model", self.models["news_classifier"])
                    gpu_mgr.register_model(f"{model_key}_tokenizer", self.tokenizers["news_classifier"])
                    logger.info("‚úÖ News Classification Model loaded and registered with GPU orchestrator")

            logger.info("‚úÖ News Classification Model ready")

        except Exception as e:
            logger.error(f"‚ùå Failed to load News Classification Model: {e}")
            self.models["news_classifier"] = None
            self.pipelines["news_classifier"] = None
            self.tokenizers["news_classifier"] = None

        try:
            # 2. Content Quality Assessment Model (BERT-based) - Production Ready
            logger.info("üî• Loading Quality Assessment Model (BERT)...")
            quality_config = self.model_configs["quality_assessor"]

            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                # Use GPU orchestrator for model management
                gpu_mgr = get_gpu_manager()
                model_key = f"scout_quality_assessor"

                # Check if model already exists in registry
                existing_pipeline = gpu_mgr.get(model_key)
                if existing_pipeline:
                    self.pipelines["quality_assessor"] = existing_pipeline
                    self.models["quality_assessor"] = True
                    self.tokenizers["quality_assessor"] = True  # Mark as available
                    logger.info("‚úÖ Quality Assessment Model loaded from GPU orchestrator registry")
                else:
                    try:
                        from agents.common.model_loader import load_transformers_model
                        model, tokenizer = load_transformers_model(
                            quality_config["model_name"],
                            agent='scout',
                            cache_dir=None,
                            model_class=AutoModelForSequenceClassification,
                            tokenizer_class=AutoTokenizer,
                        )
                        try:
                            model = model.to(self.device)
                        except Exception:
                            pass
                        self.models["quality_assessor"] = model
                        self.tokenizers["quality_assessor"] = tokenizer
                    except Exception:
                        self.models["quality_assessor"] = AutoModelForSequenceClassification.from_pretrained(
                            quality_config["model_name"],
                            num_labels=quality_config["num_labels"],
                            dtype=torch.float16 if self.device.startswith("cuda") else torch.float32,
                            ignore_mismatched_sizes=True,  # Allow different number of labels
                            trust_remote_code=True
                        ).to(self.device)

                        self.tokenizers["quality_assessor"] = AutoTokenizer.from_pretrained(
                            quality_config["model_name"],
                            trust_remote_code=True,
                            use_fast=True
                        )

                    self.pipelines["quality_assessor"] = pipeline(
                        "text-classification",
                        model=self.models["quality_assessor"],
                        tokenizer=self.tokenizers["quality_assessor"],
                        revision=quality_config.get("revision", "main"),  # Use revision for production stability
                        device=0 if self.device.startswith("cuda") else -1,
                        top_k=None,  # Updated API
                        batch_size=1,
                        truncation=True,
                        max_length=512
                    )

                    # Register with GPU orchestrator
                    gpu_mgr.register_model(model_key, self.pipelines["quality_assessor"])
                    gpu_mgr.register_model(f"{model_key}_model", self.models["quality_assessor"])
                    gpu_mgr.register_model(f"{model_key}_tokenizer", self.tokenizers["quality_assessor"])
                    logger.info("‚úÖ Quality Assessment Model loaded and registered with GPU orchestrator")

            logger.info("‚úÖ Quality Assessment Model ready")

        except Exception as e:
            logger.error(f"‚ùå Failed to load Quality Assessment Model: {e}")
            self.models["quality_assessor"] = None
            self.pipelines["quality_assessor"] = None
            self.tokenizers["quality_assessor"] = None

        try:
            # 3. Sentiment Analysis Model (RoBERTa) - Production Ready
            logger.info("üî• Loading Sentiment Analysis Model (RoBERTa)...")
            sentiment_config = self.model_configs["sentiment_analyzer"]

            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                # Use GPU orchestrator for model management
                gpu_mgr = get_gpu_manager()
                model_key = f"scout_sentiment_analyzer"

                # Check if model already exists in registry
                existing_pipeline = gpu_mgr.get(model_key)
                if existing_pipeline:
                    self.pipelines["sentiment_analyzer"] = existing_pipeline
                    self.models["sentiment_analyzer"] = True
                    logger.info("‚úÖ Sentiment Analysis Model loaded from GPU orchestrator registry")
                else:
                    if PIPELINE_AVAILABLE:
                        # Load sentiment analysis pipeline directly (optimized)
                        pipeline_obj = pipeline(
                            "sentiment-analysis",
                            model=sentiment_config["model_name"],
                            revision=sentiment_config.get("revision", "main"),  # Use revision for production stability
                            device=0 if self.device.startswith("cuda") else -1,
                            dtype=torch.float16 if self.device.startswith("cuda") else torch.float32,
                            top_k=None,  # Updated API
                            batch_size=sentiment_config["batch_size"],
                            truncation=True,
                            max_length=512
                        )

                        # Register with GPU orchestrator
                        gpu_mgr.register_model(model_key, pipeline_obj)
                        self.pipelines["sentiment_analyzer"] = pipeline_obj
                        self.models["sentiment_analyzer"] = True
                        logger.info("‚úÖ Sentiment Analysis Model loaded and registered with GPU orchestrator")
                    else:
                        logger.warning("‚ö†Ô∏è Sentiment analysis pipeline not available due to transformers compatibility issue")
                        self.models["sentiment_analyzer"] = None
                        self.pipelines["sentiment_analyzer"] = None

            if self.models["sentiment_analyzer"]:
                logger.info("‚úÖ Sentiment Analysis Model ready")
            else:
                logger.warning("‚ö†Ô∏è Sentiment Analysis Model not available")

        except Exception as e:
            logger.error(f"‚ùå Failed to load Sentiment Analysis Model: {e}")
            self.models["sentiment_analyzer"] = None
            self.pipelines["sentiment_analyzer"] = None

        try:
            # 4. Bias Detection Model (Specialized) - Production Ready
            logger.info("üî• Loading Bias Detection Model (Specialized)...")
            bias_config = self.model_configs["bias_detector"]

            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                # Use GPU orchestrator for model management
                gpu_mgr = get_gpu_manager()
                model_key = f"scout_bias_detector"

                # Check if model already exists in registry
                existing_pipeline = gpu_mgr.get(model_key)
                if existing_pipeline:
                    self.pipelines["bias_detector"] = existing_pipeline
                    self.models["bias_detector"] = True
                    self.tokenizers["bias_detector"] = True  # Mark as available
                    logger.info("‚úÖ Bias Detection Model loaded from GPU orchestrator registry")
                else:
                    try:
                        from agents.common.model_loader import load_transformers_model
                        model, tokenizer = load_transformers_model(
                            bias_config["model_name"],
                            agent='scout',
                            cache_dir=None,
                            model_class=AutoModelForSequenceClassification,
                            tokenizer_class=AutoTokenizer,
                        )
                        try:
                            model = model.to(self.device)
                        except Exception:
                            pass
                        self.models["bias_detector"] = model
                        self.tokenizers["bias_detector"] = tokenizer
                        self.pipelines["bias_detector"] = pipeline(
                            "text-classification",
                            model=self.models["bias_detector"],
                            tokenizer=self.tokenizers["bias_detector"],
                            revision=bias_config.get("revision", "main"),  # Use revision for production stability
                            device=0 if self.device.startswith("cuda") else -1,
                            top_k=None,
                            batch_size=1,
                            truncation=True,
                            max_length=512
                        )

                        # Register with GPU orchestrator
                        gpu_mgr.register_model(model_key, self.pipelines["bias_detector"])
                        gpu_mgr.register_model(f"{model_key}_model", model)
                        gpu_mgr.register_model(f"{model_key}_tokenizer", tokenizer)
                        logger.info("‚úÖ Bias Detection Model loaded and registered with GPU orchestrator")
                    except Exception:
                        self.models["bias_detector"] = AutoModelForSequenceClassification.from_pretrained(
                            bias_config["model_name"],
                            num_labels=bias_config["num_labels"],
                            dtype=torch.float16 if self.device.startswith("cuda") else torch.float32,
                            trust_remote_code=True
                        ).to(self.device)

                        self.tokenizers["bias_detector"] = AutoTokenizer.from_pretrained(
                            bias_config["model_name"],
                            trust_remote_code=True,
                            use_fast=True
                        )

                        self.pipelines["bias_detector"] = pipeline(
                            "text-classification",
                            model=self.models["bias_detector"],
                            tokenizer=self.tokenizers["bias_detector"],
                            revision=bias_config.get("revision", "main"),  # Use revision for production stability
                            device=0 if self.device.startswith("cuda") else -1,
                            top_k=None,
                            batch_size=1,
                            truncation=True,
                            max_length=512
                        )

                        # Register with GPU orchestrator
                        gpu_mgr.register_model(model_key, self.pipelines["bias_detector"])
                        gpu_mgr.register_model(f"{model_key}_model", self.models["bias_detector"])
                        gpu_mgr.register_model(f"{model_key}_tokenizer", self.tokenizers["bias_detector"])
                        logger.info("‚úÖ Bias Detection Model loaded and registered with GPU orchestrator")

            logger.info("‚úÖ Bias Detection Model ready")

        except Exception as e:
            logger.error(f"‚ùå Failed to load Bias Detection Model: {e}")
            self.models["bias_detector"] = None
            self.pipelines["bias_detector"] = None
            self.tokenizers["bias_detector"] = None

        try:
            # 5. Visual Analysis Model (LLaVA) - Production Ready
            logger.info("üî• Loading Visual Analysis Model (LLaVA)...")
            visual_config = self.model_configs["visual_analyzer"]

            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                if LLAVA_AVAILABLE and LlavaNextProcessor and LlavaNextForConditionalGeneration:
                    try:
                        from agents.common.model_loader import load_transformers_model
                        model, processor = load_transformers_model(
                            visual_config["model_name"],
                            agent='scout',
                            cache_dir=None,
                            model_class=LlavaNextForConditionalGeneration,
                            tokenizer_class=LlavaNextProcessor,
                        )
                        try:
                            model = model.to(self.device)
                        except Exception:
                            pass
                        self.models["visual_analyzer"] = model
                        self.tokenizers["visual_analyzer"] = processor
                    except Exception:
                        self.models["visual_analyzer"] = LlavaNextForConditionalGeneration.from_pretrained(
                            visual_config["model_name"],
                            dtype=torch.float16 if self.device.startswith("cuda") else torch.float32,
                            low_cpu_mem_usage=True,
                            trust_remote_code=True
                        ).to(self.device)

                        self.tokenizers["visual_analyzer"] = LlavaNextProcessor.from_pretrained(
                            visual_config["model_name"],
                            trust_remote_code=True,
                            use_fast=True  # Use fast processor to avoid warnings
                        )

                    logger.info("‚úÖ Visual Analysis Model (LLaVA) loaded successfully")
                else:
                    logger.warning("‚ö†Ô∏è LLaVA models not available due to transformers compatibility issue")
                    self.models["visual_analyzer"] = None
                    self.tokenizers["visual_analyzer"] = None

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Visual Analysis Model not available: {e}")
            self.models["visual_analyzer"] = None

        # Model registration is handled individually in each loading section above
        logger.info("üéØ Scout GPU orchestrator integration complete")

    def classify_news_content(self, text: str, url: str = "", use_ensemble: bool = True) -> dict[str, Any]:
        """
        AI-First news content classification using specialized BERT model
        Production-ready with comprehensive error handling
        
        Args:
            text: Content text to classify
            url: Optional URL for context
            use_ensemble: Use ensemble prediction for higher accuracy
        
        Returns:
            Classification results with confidence scores
        """
        try:
            if not self.models.get("news_classifier"):
                logger.debug("News classification model not available, using fallback")
                return self._emergency_fallback(text, url)

            # Primary AI Classification with production-ready handling
            logger.debug("ü§ñ Running AI-first news classification...")

            # Prepare input text with URL context if available
            input_text = f"URL: {url}\nContent: {text}" if url else text
            input_text = input_text[:512]  # Truncate to model limits

            # Suppress warnings during inference for clean operation
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                # Get predictions from the specialized news model
                if self.pipelines["news_classifier"]:
                    predictions = self.pipelines["news_classifier"](input_text)
                else:
                    logger.debug("News classifier pipeline not available")
                    return self._emergency_fallback(text, url)

            # Process results - handle both old and new pipeline formats
            news_score = 0.0
            non_news_score = 0.0

            if isinstance(predictions, list):
                for pred in predictions:
                    if isinstance(pred, dict) and 'label' in pred and 'score' in pred:
                        if pred['label'].upper() in ['LABEL_1', 'NEWS', 'TRUE', '1', 'POSITIVE']:
                            news_score = max(news_score, pred['score'])
                        else:
                            non_news_score = max(non_news_score, pred['score'])
            else:
                # Single prediction format
                if predictions.get('label', '').upper() in ['LABEL_1', 'NEWS', 'TRUE', '1', 'POSITIVE']:
                    news_score = predictions.get('score', 0.0)
                else:
                    non_news_score = predictions.get('score', 0.0)

            # Determine classification
            is_news = news_score > (non_news_score * 0.9)  # Lower the bar for news classification
            confidence = max(news_score, non_news_score)

            # Content type classification
            if is_news and confidence > 0.8:
                content_type = "news"
            elif is_news and confidence > 0.6:
                content_type = "likely_news"
            elif not is_news and confidence > 0.8:
                content_type = "non_news"
            else:
                content_type = "uncertain"

            result = {
                "is_news": is_news,
                "confidence": float(confidence),
                "content_type": content_type,
                "reasoning": f"AI classification using specialized BERT model (news_score: {news_score:.3f}, non_news_score: {non_news_score:.3f})",
                "method": "ai_bert_specialized",
                "model_used": self.model_configs["news_classifier"]["model_name"],
                "url": url,
                "timestamp": datetime.now(UTC).isoformat(),
                "predictions": predictions
            }

            # Ensemble enhancement if enabled
            if use_ensemble and confidence < 0.9:
                result = self._enhance_with_quality_signals(result, text, url)

            logger.debug(f"‚úÖ AI News Classification: {result['is_news']} ({result['confidence']:.2f})")
            return result

        except Exception as e:
            logger.warning(f"AI news classification failed, using fallback: {e}")
            return self._emergency_fallback(text, url)

    def assess_content_quality(self, text: str, url: str = "") -> dict[str, Any]:
        """
        AI-powered content quality assessment using BERT model
        Production-ready with warning suppression
        
        Args:
            text: Content to assess
            url: Optional URL context
            
        Returns:
            Quality assessment scores and metrics
        """
        try:
            if not self.models.get("quality_assessor"):
                logger.debug("Quality assessment model not available, using heuristics")
                return self._heuristic_quality_assessment(text, url)

            logger.debug("üîç Running AI quality assessment...")

            # Prepare input for quality assessment
            input_text = text[:512]

            # Get quality predictions with warning suppression
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                if self.pipelines["quality_assessor"]:
                    predictions = self.pipelines["quality_assessor"](input_text)
                else:
                    logger.debug("Quality assessor pipeline not available")
                    return self._heuristic_quality_assessment(text, url)

            # Process quality scores - handle both formats
            quality_scores = {}
            max_score = 0.0
            quality_label = "low"

            if isinstance(predictions, list):
                for pred in predictions:
                    if isinstance(pred, dict) and 'label' in pred and 'score' in pred:
                        score = pred['score']
                        label = pred['label'].lower()
                        quality_scores[label] = score

                        if score > max_score:
                            max_score = score
                            quality_label = label
            else:
                # Single prediction format
                max_score = predictions.get('score', 0.0)
                quality_label = predictions.get('label', 'low').lower()
                quality_scores[quality_label] = max_score

            # Map to standardized quality metrics
            if quality_label in ['high', 'positive', 'label_2']:
                overall_quality = min(max_score + 0.1, 1.0)
                quality_rating = "high"
            elif quality_label in ['medium', 'neutral', 'label_1']:
                overall_quality = max(min(max_score, 0.8), 0.4)
                quality_rating = "medium"
            else:
                overall_quality = max(max_score - 0.1, 0.0)
                quality_rating = "low"

            # Additional quality metrics
            word_count = len(text.split())
            sentence_count = len([s for s in text.split('.') if s.strip()])
            avg_sentence_length = word_count / max(sentence_count, 1)

            result = {
                "overall_quality": float(overall_quality),
                "quality_rating": quality_rating,
                "confidence": float(max_score),
                "factual_content": float(overall_quality),  # Placeholder - can be improved
                "writing_quality": float(min(overall_quality + 0.1, 1.0)),
                "completeness": float(min(word_count / 200, 1.0)),  # Based on length
                "readability": float(max(1.0 - (avg_sentence_length / 30), 0.3)),
                "reasoning": f"AI quality assessment using DeBERTa model (rating: {quality_rating}, confidence: {max_score:.3f})",
                "method": "ai_deberta_quality",
                "model_used": self.model_configs["quality_assessor"]["model_name"],
                "metrics": {
                    "word_count": word_count,
                    "sentence_count": sentence_count,
                    "avg_sentence_length": avg_sentence_length
                },
                "url": url,
                "timestamp": datetime.now(UTC).isoformat(),
                "raw_predictions": predictions
            }

            logger.debug(f"‚úÖ AI Quality Assessment: {quality_rating} ({overall_quality:.2f})")
            return result

        except Exception as e:
            logger.debug(f"AI quality assessment failed, using heuristics: {e}")
            return self._heuristic_quality_assessment(text, url)

    def analyze_sentiment(self, text: str, url: str = "") -> dict[str, Any]:
        """
        AI-powered sentiment analysis using specialized RoBERTa model
        Production-ready with comprehensive error handling
        
        Args:
            text: Content to analyze for sentiment
            url: Optional URL context
            
        Returns:
            Sentiment analysis results and scores
        """
        try:
            if not self.models.get("sentiment_analyzer"):
                logger.debug("Sentiment analysis model not available, using heuristics")
                return self._heuristic_sentiment_analysis(text, url)

            logger.debug("üòä Running AI sentiment analysis...")

            # Prepare input for sentiment analysis
            input_text = text[:512]

            # Get sentiment predictions with warning suppression
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                if self.pipelines["sentiment_analyzer"]:
                    predictions = self.pipelines["sentiment_analyzer"](input_text)
                else:
                    logger.debug("Sentiment analyzer pipeline not available")
                    return self._heuristic_sentiment_analysis(text, url)

            # Process sentiment scores
            sentiment_scores = {
                'positive': 0.0,
                'negative': 0.0,
                'neutral': 0.0
            }

            dominant_sentiment = "neutral"
            confidence = 0.0

            if isinstance(predictions, list):
                for pred in predictions:
                    if isinstance(pred, dict) and 'label' in pred and 'score' in pred:
                        label = pred['label'].lower()
                        score = pred['score']

                        if label in ['positive', 'pos', 'label_2']:
                            sentiment_scores['positive'] = max(sentiment_scores['positive'], score)
                        elif label in ['negative', 'neg', 'label_0']:
                            sentiment_scores['negative'] = max(sentiment_scores['negative'], score)
                        else:
                            sentiment_scores['neutral'] = max(sentiment_scores['neutral'], score)
            else:
                # Single prediction format
                label = predictions.get('label', '').lower()
                score = predictions.get('score', 0.0)

                if label in ['positive', 'pos', 'label_2']:
                    sentiment_scores['positive'] = score
                elif label in ['negative', 'neg', 'label_0']:
                    sentiment_scores['negative'] = score
                else:
                    sentiment_scores['neutral'] = score

            # Determine dominant sentiment
            dominant_sentiment = max(sentiment_scores, key=sentiment_scores.get)
            confidence = sentiment_scores[dominant_sentiment]

            # Sentiment intensity categorization
            if confidence > 0.8:
                intensity = "strong"
            elif confidence > 0.6:
                intensity = "moderate"
            elif confidence > 0.4:
                intensity = "mild"
            else:
                intensity = "weak"

            result = {
                "dominant_sentiment": dominant_sentiment,
                "confidence": float(confidence),
                "intensity": intensity,
                "sentiment_scores": {k: float(v) for k, v in sentiment_scores.items()},
                "method": "ai_roberta_specialized",
                "model_name": self.model_configs["sentiment_analyzer"]["model_name"],
                "url": url,
                "analysis_timestamp": datetime.now(UTC).isoformat(),
                "raw_predictions": predictions
            }

            logger.debug(f"‚úÖ AI Sentiment Analysis: {dominant_sentiment} ({confidence:.2f})")
            return result

        except Exception as e:
            logger.debug(f"AI sentiment analysis failed, using heuristics: {e}")
            return self._heuristic_sentiment_analysis(text, url)

    def detect_bias(self, text: str, url: str = "") -> dict[str, Any]:
        """
        AI-powered bias detection using specialized classification model
        Production-ready with comprehensive error handling
        
        Args:
            text: Content to analyze for bias
            url: Optional URL context
            
        Returns:
            Bias detection results and scores
        """
        try:
            if not self.models.get("bias_detector"):
                logger.debug("Bias detection model not available, using heuristics")
                return self._heuristic_bias_detection(text, url)

            logger.debug("‚öñÔ∏è Running AI bias detection...")

            # Prepare input for bias detection
            input_text = text[:512]

            # Get bias predictions with warning suppression
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                if self.pipelines["bias_detector"]:
                    predictions = self.pipelines["bias_detector"](input_text)
                else:
                    logger.debug("Bias detector pipeline not available")
                    return self._heuristic_bias_detection(text, url)

            # Process bias scores - handle both formats
            bias_score = 0.0
            non_bias_score = 0.0

            if isinstance(predictions, list):
                for pred in predictions:
                    if isinstance(pred, dict) and 'label' in pred and 'score' in pred:
                        if pred['label'].upper() in ['TOXIC', 'BIAS', 'LABEL_1', 'POSITIVE']:
                            bias_score = max(bias_score, pred['score'])
                        else:
                            non_bias_score = max(non_bias_score, pred['score'])
            else:
                # Single prediction format
                if predictions.get('label', '').upper() in ['TOXIC', 'BIAS', 'LABEL_1', 'POSITIVE']:
                    bias_score = predictions.get('score', 0.0)
                else:
                    non_bias_score = predictions.get('score', 0.0)

            # Determine bias level
            has_bias = bias_score > non_bias_score
            confidence = max(bias_score, non_bias_score)

            # Bias categorization
            if bias_score > 0.8:
                bias_level = "high"
            elif bias_score > 0.6:
                bias_level = "medium"
            elif bias_score > 0.3:
                bias_level = "low"
            else:
                bias_level = "minimal"

            result = {
                "has_bias": has_bias,
                "bias_score": float(bias_score),
                "bias_level": bias_level,
                "confidence": float(confidence),
                "political_bias": float(bias_score * 0.7),  # Estimate
                "emotional_bias": float(bias_score * 0.8),  # Estimate
                "factual_bias": float(bias_score * 0.6),   # Estimate
                "reasoning": f"AI bias detection using specialized model (bias_score: {bias_score:.3f}, level: {bias_level})",
                "method": "ai_specialized_bias",
                "model_used": self.model_configs["bias_detector"]["model_name"],
                "url": url,
                "timestamp": datetime.now(UTC).isoformat(),
                "raw_predictions": predictions
            }

            logger.debug(f"‚úÖ AI Bias Detection: {bias_level} bias ({bias_score:.2f})")
            return result

        except Exception as e:
            logger.debug(f"AI bias detection failed, using heuristics: {e}")
            return self._heuristic_bias_detection(text, url)

    def analyze_visual_content(self, image_path: str, prompt: str = "Describe this image and determine if it contains news-worthy content") -> dict[str, Any]:
        """
        AI-powered visual content analysis using local LLaVA model
        
        Args:
            image_path: Path to the image file
            prompt: Analysis prompt for the model
            
        Returns:
            Visual analysis results
        """
        try:
            if not self.models.get("visual_analyzer"):
                logger.warning("‚ö†Ô∏è Visual analysis model (LLaVA) not available")
                return {
                    "visual_analysis": "Visual analysis not available - LLaVA model not loaded",
                    "is_news_visual": False,
                    "confidence": 0.0,
                    "method": "unavailable"
                }

            from PIL import Image

            logger.debug("üëÅÔ∏è Running AI visual analysis...")

            # Load and process image
            image = Image.open(image_path)
            processor = self.tokenizers["visual_analyzer"]
            model = self.models["visual_analyzer"]

            # Prepare inputs
            inputs = processor(prompt, image, return_tensors="pt").to(self.device)

            # Generate analysis
            with torch.no_grad():
                output = model.generate(
                    **inputs,
                    max_new_tokens=self.model_configs["visual_analyzer"]["max_new_tokens"],
                    do_sample=True,
                    temperature=0.1
                )

            # Decode response
            response = processor.decode(output[0], skip_special_tokens=True)
            analysis = response[len(prompt):].strip()

            # Determine if visual content is news-worthy
            news_keywords = ['news', 'breaking', 'event', 'incident', 'report', 'announcement']
            is_news_visual = any(keyword in analysis.lower() for keyword in news_keywords)

            # Calculate confidence based on response quality
            confidence = min(len(analysis) / 100, 1.0)

            result = {
                "visual_analysis": analysis,
                "is_news_visual": is_news_visual,
                "confidence": float(confidence),
                "method": "ai_llava_visual",
                "model_used": self.model_configs["visual_analyzer"]["model_name"],
                "image_path": image_path,
                "timestamp": datetime.now(UTC).isoformat(),
                "prompt_used": prompt
            }

            logger.debug(f"‚úÖ AI Visual Analysis completed: {is_news_visual}")
            return result

        except Exception as e:
            logger.error(f"‚ùå AI visual analysis failed: {e}")
            return {
                "visual_analysis": f"Visual analysis failed: {str(e)}",
                "is_news_visual": False,
                "confidence": 0.0,
                "method": "error"
            }

    def comprehensive_content_analysis(self, text: str, url: str = "", image_path: str = None) -> dict[str, Any]:
        """
        Complete AI-powered content analysis using all specialized models
        
        Args:
            text: Content text to analyze
            url: Optional URL context
            image_path: Optional image for visual analysis
            
        Returns:
            Comprehensive analysis results
        """
        logger.info("üîç Running comprehensive AI content analysis...")

        # Run all AI analyses
        news_analysis = self.classify_news_content(text, url, use_ensemble=True)
        quality_analysis = self.assess_content_quality(text, url)
        sentiment_analysis = self.analyze_sentiment(text, url)
        bias_analysis = self.detect_bias(text, url)

        # Visual analysis if image provided
        visual_analysis = None
        if image_path and os.path.exists(image_path):
            visual_analysis = self.analyze_visual_content(image_path)

        # Calculate overall Scout score
        scout_score = self._calculate_comprehensive_scout_score(
            news_analysis, quality_analysis, sentiment_analysis, bias_analysis, visual_analysis
        )

        # Generate recommendation
        recommendation = self._generate_comprehensive_recommendation(scout_score, news_analysis, quality_analysis, sentiment_analysis, bias_analysis)

        result = {
            "scout_score": scout_score,
            "recommendation": recommendation,
            "news_classification": news_analysis,
            "quality_assessment": quality_analysis,
            "sentiment_analysis": sentiment_analysis,
            "bias_detection": bias_analysis,
            "visual_analysis": visual_analysis,
            "analysis_timestamp": datetime.now(UTC).isoformat(),
            "models_used": [config["model_name"] for config in self.model_configs.values() if self.models.get(config["task"].split("_")[0] + "_" + config["task"].split("_")[1])],
            "ai_first_approach": True
        }

        logger.info(f"‚úÖ Comprehensive analysis completed. Scout Score: {scout_score:.2f}")
        return result

    def add_training_example(self, task: str, text: str, label: int | str, url: str = ""):
        """
        Add training example for continuous learning
        
        Args:
            task: Training task ('news_classification', 'quality_assessment', 'sentiment_analysis', 'bias_detection')
            text: Input text
            label: Ground truth label
            url: Optional URL context
        """
        if not self.enable_training:
            logger.warning("‚ö†Ô∏è Training not enabled")
            return

        if task not in self.training_data:
            logger.error(f"‚ùå Unknown training task: {task}")
            return

        # Convert string labels to integers if needed
        if isinstance(label, str):
            if task == "news_classification":
                label = 1 if label.lower() in ['news', 'true', '1', 'yes'] else 0
            elif task == "quality_assessment":
                label_map = {'low': 0, 'medium': 1, 'high': 2}
                label = label_map.get(label.lower(), 1)
            elif task == "sentiment_analysis":
                label_map = {'negative': 0, 'neutral': 1, 'positive': 2}
                label = label_map.get(label.lower(), 1)
            elif task == "bias_detection":
                label = 1 if label.lower() in ['bias', 'biased', 'true', '1', 'yes'] else 0

        # Add to training data
        input_text = f"URL: {url}\nContent: {text}" if url else text
        self.training_data[task]["texts"].append(input_text)
        self.training_data[task]["labels"].append(label)

        logger.debug(f"üìö Added training example for {task}: {len(self.training_data[task]['texts'])} examples")

    def fine_tune_model(self, task: str, epochs: int = 3, learning_rate: float = 2e-5, batch_size: int = 8):
        """
        Fine-tune a specific model with collected training data
        
        Args:
            task: Model to fine-tune ('news_classification', 'quality_assessment', 'bias_detection')
            epochs: Number of training epochs
            learning_rate: Learning rate for training
            batch_size: Training batch size
        """
        if not self.enable_training:
            logger.error("‚ùå Training not enabled")
            return False

        if task not in self.training_data:
            logger.error(f"‚ùå Unknown training task: {task}")
            return False

        training_texts = self.training_data[task]["texts"]
        training_labels = self.training_data[task]["labels"]

        if len(training_texts) < 10:
            logger.warning(f"‚ö†Ô∏è Insufficient training data for {task}: {len(training_texts)} examples")
            return False

        try:
            logger.info(f"üèãÔ∏è Fine-tuning {task} model with {len(training_texts)} examples...")

            # Get model and tokenizer
            model_key = task.split("_")[0] + "_" + task.split("_")[1]
            if not self.models.get(model_key):
                logger.error(f"‚ùå Model not available for {task}")
                return False

            model = self.models[model_key]
            tokenizer = self.tokenizers[model_key]

            # Create dataset
            dataset = OptimizedNewsDataset(training_texts, training_labels, tokenizer)

            # Training arguments
            training_args = TrainingArguments(
                output_dir=f"./fine_tuned_{task}",
                num_train_epochs=epochs,
                per_device_train_batch_size=batch_size,
                learning_rate=learning_rate,
                logging_steps=10,
                save_steps=100,
                evaluation_strategy="no",
                save_total_limit=2,
                remove_unused_columns=False,
            )

            # Initialize trainer
            if TRAINER_AVAILABLE and Trainer:
                trainer = Trainer(
                    model=model,
                    args=training_args,
                    train_dataset=dataset,
                    tokenizer=tokenizer,
                )

                # Fine-tune
                trainer.train()

                # Save fine-tuned model
                model_save_path = f"./fine_tuned_{task}_model"
                model.save_pretrained(model_save_path)
                tokenizer.save_pretrained(model_save_path)

                logger.info(f"‚úÖ Fine-tuning completed for {task}. Model saved to {model_save_path}")

                # Update pipeline with fine-tuned model
                if PIPELINE_AVAILABLE:
                    self.pipelines[model_key] = pipeline(
                        "text-classification",
                        model=model,
                        tokenizer=tokenizer,
                        revision=self.model_configs.get(task, {}).get("revision", "main"),  # Use revision for production stability
                        device=0 if self.device.startswith("cuda") else -1,
                        top_k=None,
                        batch_size=1,
                        truncation=True,
                        max_length=512
                    )

                return True
            else:
                logger.warning(f"‚ö†Ô∏è Trainer not available for {task} fine-tuning due to transformers compatibility issue")
                return False

        except Exception as e:
            logger.error(f"‚ùå Fine-tuning failed for {task}: {e}")
            return False

    def _enhance_with_quality_signals(self, base_result: dict, text: str, url: str) -> dict:
        """Enhance classification with quality signals for borderline cases"""
        try:
            quality_result = self.assess_content_quality(text, url)

            # Adjust confidence based on quality
            quality_bonus = quality_result.get("overall_quality", 0.5) * 0.1
            base_result["confidence"] = min(base_result["confidence"] + quality_bonus, 1.0)
            base_result["reasoning"] += f" + Quality enhancement ({quality_result.get('overall_quality', 0):.2f})"

            return base_result

        except Exception as e:
            logger.debug(f"Quality enhancement failed: {e}")
            return base_result

    def _calculate_comprehensive_scout_score(self, news_analysis: dict, quality_analysis: dict, sentiment_analysis: dict, bias_analysis: dict, visual_analysis: dict = None) -> float:
        """Calculate comprehensive Scout score from all analyses"""
        try:
            # Base news confidence
            news_score = news_analysis.get("confidence", 0.0) if news_analysis.get("is_news", False) else 0.0

            # Quality weighted score
            quality_score = quality_analysis.get("overall_quality", 0.0)

            # Sentiment neutrality bonus (neutral sentiment is preferred for news)
            sentiment_bonus = 0.0
            dominant_sentiment = sentiment_analysis.get("dominant_sentiment", "neutral")
            sentiment_confidence = sentiment_analysis.get("confidence", 0.5)

            if dominant_sentiment == "neutral":
                sentiment_bonus = 0.2  # Neutral sentiment is ideal for news
            elif dominant_sentiment in ["positive", "negative"] and sentiment_confidence < 0.7:
                sentiment_bonus = 0.1  # Mild sentiment is acceptable
            else:
                sentiment_bonus = -0.1  # Strong sentiment may indicate bias

            # Bias penalty (high bias reduces score)
            bias_penalty = 1.0 - bias_analysis.get("bias_score", 0.5)

            # Visual enhancement if available
            visual_bonus = 0.0
            if visual_analysis and visual_analysis.get("is_news_visual"):
                visual_bonus = visual_analysis.get("confidence", 0.0) * 0.1

            # Combined Scout score with sentiment consideration
            scout_score = (news_score * 0.35 + quality_score * 0.25 + bias_penalty * 0.2 + sentiment_bonus * 0.15 + visual_bonus * 0.05)

            return min(max(scout_score, 0.0), 1.0)  # Clamp to [0, 1]

        except Exception as e:
            logger.error(f"Scout score calculation error: {e}")
            return 0.5

    def _generate_comprehensive_recommendation(self, scout_score: float, news_analysis: dict, quality_analysis: dict, sentiment_analysis: dict, bias_analysis: dict) -> str:
        """Generate recommendation based on comprehensive analysis"""

        # Get key indicators
        is_news = news_analysis.get("is_news", False)
        quality_level = quality_analysis.get("quality_rating", "low")
        sentiment = sentiment_analysis.get("dominant_sentiment", "neutral")
        sentiment_intensity = sentiment_analysis.get("intensity", "mild")
        bias_level = bias_analysis.get("bias_level", "minimal")

        # Generate context-aware recommendation
        if scout_score >= 0.8:
            context = []
            if is_news and quality_level == "high":
                context.append("high-quality news")
            if sentiment == "neutral":
                context.append("neutral tone")
            if bias_level == "minimal":
                context.append("minimal bias")

            return f"üî• HIGH_PRIORITY: Excellent content ({', '.join(context)})"

        elif scout_score >= 0.6:
            context = []
            if sentiment != "neutral" and sentiment_intensity != "weak":
                context.append(f"{sentiment_intensity} {sentiment}")
            if bias_level != "minimal":
                context.append(f"{bias_level} bias")

            context_str = f" ({', '.join(context)})" if context else ""
            return f"üëç MEDIUM_PRIORITY: Good quality news content{context_str}"

        elif scout_score >= 0.4:
            issues = []
            if not is_news:
                issues.append("questionable news classification")
            if quality_level == "low":
                issues.append("low quality")
            if sentiment != "neutral" and sentiment_intensity in ["strong", "moderate"]:
                issues.append(f"strong {sentiment} sentiment")
            if bias_level in ["high", "medium"]:
                issues.append(f"{bias_level} bias detected")

            issues_str = f" ({', '.join(issues)})" if issues else ""
            return f"‚ö†Ô∏è LOW_PRIORITY: Borderline content{issues_str}, manual review recommended"
        else:
            problems = []
            if not is_news:
                problems.append("non-news content")
            if quality_level == "low":
                problems.append("poor quality")
            if bias_level == "high":
                problems.append("high bias")
            if sentiment == "negative" and sentiment_intensity == "strong":
                problems.append("strongly negative")

            problems_str = f" ({', '.join(problems)})" if problems else ""
            return f"‚ùå REJECT: Poor quality or problematic content{problems_str}, exclude from pipeline"

    def _emergency_fallback(self, text: str, url: str) -> dict:
        """Emergency fallback when AI models fail"""
        # Simple keyword-based fallback
        news_keywords = ['breaking', 'news', 'reported', 'announced', 'according', 'sources']
        news_count = sum(1 for keyword in news_keywords if keyword.lower() in text.lower())

        is_news = news_count >= 2 or len(text.split()) > 100
        confidence = min(0.6, news_count * 0.15 + 0.3)

        return {
            "is_news": is_news,
            "confidence": confidence,
            "content_type": "news" if is_news else "unknown",
            "reasoning": f"Emergency fallback classification (keywords: {news_count})",
            "method": "emergency_fallback",
            "url": url,
            "timestamp": datetime.now(UTC).isoformat()
        }

    def _heuristic_quality_assessment(self, text: str, url: str) -> dict:
        """Heuristic quality assessment fallback"""
        word_count = len(text.split())
        sentence_count = len([s for s in text.split('.') if s.strip()])

        # Simple quality heuristics
        if word_count > 300 and sentence_count > 5:
            quality = 0.8
            rating = "high"
        elif word_count > 100 and sentence_count > 3:
            quality = 0.6
            rating = "medium"
        else:
            quality = 0.4
            rating = "low"

        return {
            "overall_quality": quality,
            "quality_rating": rating,
            "confidence": 0.6,
            "reasoning": f"Heuristic assessment based on length (words: {word_count})",
            "method": "heuristic_fallback"
        }

    def _heuristic_bias_detection(self, text: str, url: str) -> dict:
        """Heuristic bias detection fallback"""
        bias_keywords = ['always', 'never', 'all', 'everyone', 'nobody', 'terrible', 'amazing', 'best', 'worst']
        bias_count = sum(1 for keyword in bias_keywords if keyword.lower() in text.lower())

        bias_score = min(bias_count * 0.1, 1.0)

        return {
            "has_bias": bias_score > 0.3,
            "bias_score": bias_score,
            "bias_level": "high" if bias_score > 0.6 else "medium" if bias_score > 0.3 else "low",
            "confidence": 0.5,
            "reasoning": f"Heuristic bias detection (bias indicators: {bias_count})",
            "method": "heuristic_fallback"
        }

    def _heuristic_sentiment_analysis(self, text: str, url: str) -> dict:
        """Heuristic sentiment analysis fallback"""
        positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'positive', 'success', 'win', 'happy', 'pleased']
        negative_words = ['bad', 'terrible', 'awful', 'horrible', 'negative', 'fail', 'loss', 'sad', 'angry', 'disappointed']

        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)

        # Calculate sentiment scores
        total_sentiment_words = positive_count + negative_count
        if total_sentiment_words == 0:
            dominant_sentiment = "neutral"
            confidence = 0.5
            sentiment_scores = {'positive': 0.33, 'negative': 0.33, 'neutral': 0.34}
        elif positive_count > negative_count:
            dominant_sentiment = "positive"
            confidence = min(0.5 + (positive_count - negative_count) * 0.1, 0.9)
            sentiment_scores = {'positive': confidence, 'negative': 1.0 - confidence, 'neutral': 0.0}
        elif negative_count > positive_count:
            dominant_sentiment = "negative"
            confidence = min(0.5 + (negative_count - positive_count) * 0.1, 0.9)
            sentiment_scores = {'negative': confidence, 'positive': 1.0 - confidence, 'neutral': 0.0}
        else:
            dominant_sentiment = "neutral"
            confidence = 0.6
            sentiment_scores = {'positive': 0.2, 'negative': 0.2, 'neutral': 0.6}

        # Determine intensity
        if confidence > 0.8:
            intensity = "strong"
        elif confidence > 0.6:
            intensity = "moderate"
        else:
            intensity = "mild"

        return {
            "dominant_sentiment": dominant_sentiment,
            "confidence": float(confidence),
            "intensity": intensity,
            "sentiment_scores": sentiment_scores,
            "method": "heuristic_fallback",
            "model_name": "heuristic_keywords",
            "url": url,
            "analysis_timestamp": datetime.now(UTC).isoformat(),
            "reasoning": f"Heuristic sentiment analysis (positive: {positive_count}, negative: {negative_count})"
        }

    def get_model_info(self) -> dict[str, Any]:
        """Get information about loaded models"""
        model_info = {}
        for task, config in self.model_configs.items():
            model_key = task.split("_")[0] + "_" + task.split("_")[1] if "_" in task else task
            model_info[task] = {
                "model_name": config["model_name"],
                "task": config["task"],
                "loaded": self.models.get(model_key) is not None,
                "device": str(self.device),
                "training_examples": len(self.training_data.get(config["task"], {}).get("texts", [])) if self.enable_training else "N/A"
            }
        return model_info

    def cleanup(self):
        """Clean up GPU memory and resources"""
        # Release GPU allocation
        if PRODUCTION_GPU_AVAILABLE and self.gpu_allocated:
            try:
                release_agent_gpu('scout')
                logger.info("‚úÖ Scout GPU allocation released")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to release Scout GPU allocation: {e}")

        if self.device.startswith("cuda"):
            torch.cuda.empty_cache()
            logger.info("üßπ GPU memory cleaned up")

        # Clear model references
        for model_name in list(self.models.keys()):
            del self.models[model_name]
        self.models.clear()

        for tokenizer_name in list(self.tokenizers.keys()):
            del self.tokenizers[tokenizer_name]
        self.tokenizers.clear()

        logger.info("üßπ Scout model cleanup completed")

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()
