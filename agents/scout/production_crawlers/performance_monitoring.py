#!/usr/bin/env python3
"""
Performance Monitoring for Unified Production Crawler

PHASE 3 ENHANCEMENT: Comprehensive performance tracking and metrics collection
for the unified production crawler with real-time monitoring and optimization.
"""

import json
import threading
import time
from collections import defaultdict, deque
from datetime import datetime
from typing import Any

from common.observability import get_logger

logger = get_logger(__name__)


class PerformanceMetrics:
    """Real-time performance metrics collection and analysis"""

    def __init__(self, max_history: int = 1000):
        self.max_history = max_history
        self.start_time = time.time()

        # Core metrics
        self.articles_processed = 0
        self.articles_successful = 0
        self.sites_crawled = 0
        self.errors_total = 0

        # Strategy metrics
        self.strategy_usage = defaultdict(int)
        self.strategy_performance = defaultdict(list)

        # Time-based metrics
        self.processing_times = deque(maxlen=max_history)
        self.articles_per_second_history = deque(maxlen=max_history)
        self.error_rate_history = deque(maxlen=max_history)

        # Site-specific metrics
        self.site_metrics = defaultdict(lambda: {
            'articles_found': 0,
            'articles_successful': 0,
            'processing_times': [],
            'last_crawl': None,
            'strategy_used': None
        })

        # Real-time monitoring
        self.monitoring_active = False
        self.monitoring_thread = None

    def record_crawl_start(self, site_config, strategy: str):
        """Record the start of a crawl operation"""
        site_key = f"{site_config.domain}_{site_config.source_id or 'unknown'}"
        self.site_metrics[site_key]['last_crawl'] = time.time()
        self.site_metrics[site_key]['strategy_used'] = strategy
        self.strategy_usage[strategy] += 1

    def record_crawl_complete(self, site_config, articles_found: int,
                            articles_successful: int, processing_time: float):
        """Record the completion of a crawl operation"""
        site_key = f"{site_config.domain}_{site_config.source_id or 'unknown'}"

        # Update totals
        self.articles_processed += articles_found
        self.articles_successful += articles_successful
        self.sites_crawled += 1

        # Update site metrics
        self.site_metrics[site_key]['articles_found'] += articles_found
        self.site_metrics[site_key]['articles_successful'] += articles_successful
        self.site_metrics[site_key]['processing_times'].append(processing_time)

        # Update time-based metrics
        self.processing_times.append(processing_time)
        if processing_time > 0:
            articles_per_second = articles_found / processing_time
            self.articles_per_second_history.append(articles_per_second)

        # Update strategy performance
        strategy = self.site_metrics[site_key]['strategy_used']
        if strategy:
            self.strategy_performance[strategy].append(articles_per_second)

    def record_error(self, error_type: str = "unknown"):
        """Record an error occurrence"""
        self.errors_total += 1

        # Update error rate history
        current_time = time.time()
        total_runtime = current_time - self.start_time
        if total_runtime > 0:
            error_rate = self.errors_total / total_runtime
            self.error_rate_history.append(error_rate)

    def get_current_metrics(self) -> dict[str, Any]:
        """Get current performance metrics"""
        current_time = time.time()
        total_runtime = current_time - self.start_time

        # Calculate rates
        articles_per_second = self.articles_processed / total_runtime if total_runtime > 0 else 0
        success_rate = self.articles_successful / self.articles_processed if self.articles_processed > 0 else 0
        error_rate = self.errors_total / total_runtime if total_runtime > 0 else 0

        # Strategy analysis
        strategy_summary = {}
        for strategy, performances in self.strategy_performance.items():
            if performances:
                strategy_summary[strategy] = {
                    'usage_count': self.strategy_usage[strategy],
                    'avg_performance': sum(performances) / len(performances),
                    'best_performance': max(performances),
                    'worst_performance': min(performances)
                }

        return {
            "timestamp": datetime.now().isoformat(),
            "total_runtime_seconds": total_runtime,
            "articles_processed": self.articles_processed,
            "articles_successful": self.articles_successful,
            "sites_crawled": self.sites_crawled,
            "errors_total": self.errors_total,
            "articles_per_second": articles_per_second,
            "success_rate": success_rate,
            "error_rate": error_rate,
            "strategy_summary": strategy_summary,
            "site_metrics": dict(self.site_metrics)
        }

    def get_recent_performance(self, window_seconds: int = 300) -> dict[str, Any]:
        """Get performance metrics for recent time window"""
        current_time = time.time()
        current_time - window_seconds

        # Filter recent processing times
        recent_times = [t for t in self.processing_times if (current_time - t) <= window_seconds]
        recent_aps = [aps for aps in self.articles_per_second_history if (current_time - aps) <= window_seconds]

        return {
            "window_seconds": window_seconds,
            "recent_processing_times": recent_times,
            "avg_processing_time": sum(recent_times) / len(recent_times) if recent_times else 0,
            "recent_articles_per_second": recent_aps,
            "avg_articles_per_second": sum(recent_aps) / len(recent_aps) if recent_aps else 0,
            "processing_count": len(recent_times)
        }

    def get_site_performance_report(self) -> list[dict[str, Any]]:
        """Get performance report for each site"""
        report = []

        for site_key, metrics in self.site_metrics.items():
            processing_times = metrics['processing_times']
            if processing_times:
                avg_time = sum(processing_times) / len(processing_times)
                total_articles = metrics['articles_found']
                success_rate = metrics['articles_successful'] / total_articles if total_articles > 0 else 0

                report.append({
                    "site_key": site_key,
                    "total_articles": total_articles,
                    "successful_articles": metrics['articles_successful'],
                    "success_rate": success_rate,
                    "avg_processing_time": avg_time,
                    "articles_per_second": total_articles / sum(processing_times) if sum(processing_times) > 0 else 0,
                    "crawl_count": len(processing_times),
                    "last_strategy": metrics['strategy_used'],
                    "last_crawl": metrics['last_crawl']
                })

        # Sort by performance (articles per second)
        report.sort(key=lambda x: x['articles_per_second'], reverse=True)
        return report

    def start_monitoring(self, interval_seconds: int = 60):
        """Start background performance monitoring"""
        if self.monitoring_active:
            return

        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop,
            args=(interval_seconds,),
            daemon=True
        )
        self.monitoring_thread.start()
        logger.info("‚úÖ Performance monitoring started")

    def stop_monitoring(self):
        """Stop background performance monitoring"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
        logger.info("üõë Performance monitoring stopped")

    def _monitoring_loop(self, interval_seconds: int):
        """Background monitoring loop"""
        while self.monitoring_active:
            try:
                metrics = self.get_current_metrics()
                logger.info(f"üìä Performance: {metrics['articles_per_second']:.2f} articles/sec, "
                          f"Success: {metrics['success_rate']:.1%}, "
                          f"Sites: {metrics['sites_crawled']}")

                # Log strategy performance
                for strategy, perf in metrics['strategy_summary'].items():
                    logger.debug(f"üéØ {strategy}: {perf['avg_performance']:.2f} articles/sec "
                               f"({perf['usage_count']} uses)")

            except Exception as e:
                logger.error(f"Error in performance monitoring: {e}")

            time.sleep(interval_seconds)

    def export_metrics(self, filepath: str):
        """Export comprehensive metrics to JSON file"""
        metrics = {
            "export_timestamp": datetime.now().isoformat(),
            "current_metrics": self.get_current_metrics(),
            "recent_performance": self.get_recent_performance(),
            "site_performance_report": self.get_site_performance_report(),
            "raw_data": {
                "processing_times": list(self.processing_times),
                "articles_per_second_history": list(self.articles_per_second_history),
                "error_rate_history": list(self.error_rate_history)
            }
        }

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2, default=str)
            logger.info(f"‚úÖ Performance metrics exported to {filepath}")

        except Exception as e:
            logger.error(f"‚ùå Failed to export metrics: {e}")


class PerformanceOptimizer:
    """Performance optimization recommendations based on metrics"""

    def __init__(self, metrics: PerformanceMetrics):
        self.metrics = metrics

    def get_optimization_recommendations(self) -> list[str]:
        """Get optimization recommendations based on current performance"""
        recommendations = []

        current_metrics = self.metrics.get_current_metrics()

        # Articles per second analysis
        aps = current_metrics['articles_per_second']
        if aps < 0.5:
            recommendations.append("‚ö†Ô∏è Low throughput detected. Consider increasing concurrent_sites or optimizing strategies.")
        elif aps > 5.0:
            recommendations.append("‚úÖ Excellent throughput! Current configuration is highly optimized.")

        # Success rate analysis
        success_rate = current_metrics['success_rate']
        if success_rate < 0.7:
            recommendations.append("‚ö†Ô∏è Low success rate detected. Check for site blocking or content extraction issues.")
        elif success_rate > 0.9:
            recommendations.append("‚úÖ High success rate! Content extraction is working well.")

        # Strategy analysis
        strategy_summary = current_metrics['strategy_summary']
        if strategy_summary:
            best_strategy = max(strategy_summary.items(), key=lambda x: x[1]['avg_performance'])
            recommendations.append(f"üéØ Best performing strategy: {best_strategy[0]} "
                                f"({best_strategy[1]['avg_performance']:.2f} articles/sec)")

            # Check strategy balance
            total_usage = sum(s['usage_count'] for s in strategy_summary.values())
            for strategy, stats in strategy_summary.items():
                usage_pct = stats['usage_count'] / total_usage if total_usage > 0 else 0
                if usage_pct < 0.1 and stats['avg_performance'] > aps * 1.2:
                    recommendations.append(f"üí° Underutilized high-performance strategy: {strategy}. "
                                        "Consider adjusting site assignments.")

        # Error rate analysis
        error_rate = current_metrics['error_rate']
        if error_rate > 0.1:  # More than 1 error per 10 seconds
            recommendations.append("‚ö†Ô∏è High error rate detected. Check logs for recurring issues.")

        # Site-specific recommendations
        site_report = self.metrics.get_site_performance_report()
        slow_sites = [s for s in site_report if s['articles_per_second'] < 0.5]
        if slow_sites:
            recommendations.append(f"üêå {len(slow_sites)} sites performing slowly. "
                                "Consider switching to ultra_fast strategy or excluding from crawl.")

        return recommendations

    def suggest_configuration_changes(self) -> dict[str, Any]:
        """Suggest configuration changes for optimization"""
        suggestions = {}

        current_metrics = self.metrics.get_current_metrics()
        aps = current_metrics['articles_per_second']

        # Concurrent sites suggestion
        if aps < 1.0:
            suggestions['concurrent_sites'] = min(5, self.metrics.sites_crawled + 1)
        elif aps > 3.0:
            suggestions['concurrent_sites'] = max(2, self.metrics.sites_crawled - 1)

        # Articles per site suggestion
        if aps < 0.8:
            suggestions['articles_per_site'] = max(10, current_metrics.get('articles_per_site', 25) - 5)
        elif aps > 2.0:
            suggestions['articles_per_site'] = min(50, current_metrics.get('articles_per_site', 25) + 5)

        # Strategy preferences
        strategy_summary = current_metrics['strategy_summary']
        if strategy_summary:
            best_strategy = max(strategy_summary.items(), key=lambda x: x[1]['avg_performance'])
            suggestions['preferred_strategy'] = best_strategy[0]

        return suggestions


# Global performance monitoring instance
_performance_monitor = PerformanceMetrics()

def get_performance_monitor() -> PerformanceMetrics:
    """Get the global performance monitor instance"""
    return _performance_monitor

def start_performance_monitoring(interval_seconds: int = 60):
    """Start global performance monitoring"""
    _performance_monitor.start_monitoring(interval_seconds)

def stop_performance_monitoring():
    """Stop global performance monitoring"""
    _performance_monitor.stop_monitoring()

def export_performance_metrics(filepath: str):
    """Export current performance metrics"""
    _performance_monitor.export_metrics(filepath)

# Export for unified crawler integration
__all__ = [
    'PerformanceMetrics',
    'PerformanceOptimizer',
    'get_performance_monitor',
    'start_performance_monitoring',
    'stop_performance_monitoring',
    'export_performance_metrics'
]
