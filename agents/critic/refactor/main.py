"""
Critic Agent - Refactored FastAPI Application

This module provides a streamlined FastAPI application for the critic agent,
focusing on content analysis, critique synthesis, and quality assessment.

Key Features:
- Content critique and synthesis analysis
- Argument structure evaluation
- Editorial consistency checking
- Logical fallacy detection
- Source credibility assessment
- MCP Bus integration for inter-agent communication

All endpoints include comprehensive error handling, validation, and logging.
"""

import asyncio
import os
from contextlib import asynccontextmanager
from datetime import datetime
from typing import Any, Dict, List, Optional

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

from common.observability import get_logger

# Import metrics library
from common.metrics import JustNewsMetrics

# Configure logging
logger = get_logger(__name__)

# Environment variables
CRITIC_AGENT_PORT = int(os.environ.get("CRITIC_AGENT_PORT", 8006))
MCP_BUS_URL = os.environ.get("MCP_BUS_URL", "http://localhost:8000")

ready = False

class MCPBusClient:
    """MCP Bus client for inter-agent communication"""
    def __init__(self, base_url: str = MCP_BUS_URL):
        self.base_url = base_url

    def register_agent(self, agent_name: str, agent_address: str, tools: List[str]):
        """Register agent with MCP Bus"""
        registration_data = {
            "name": agent_name,
            "address": agent_address,
        }
        try:
            import requests
            response = requests.post(f"{self.base_url}/register", json=registration_data, timeout=(2, 5))
            response.raise_for_status()
            logger.info(f"Successfully registered {agent_name} with MCP Bus.")
        except Exception as e:
            logger.error(f"Failed to register {agent_name} with MCP Bus: {e}")
            raise

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager"""
    logger.info("Critic agent is starting up.")
    mcp_bus_client = MCPBusClient()
    try:
        mcp_bus_client.register_agent(
            agent_name="critic",
            agent_address=f"http://localhost:{CRITIC_AGENT_PORT}",
            tools=[
                "critique_synthesis",
                "critique_neutrality",
                "analyze_argument_structure",
                "assess_editorial_consistency",
                "detect_logical_fallacies",
                "assess_source_credibility",
                "health_check",
                "validate_critique_result",
                "format_critique_output",
                "get_critic_engine"
            ],
        )
        logger.info("Registered tools with MCP Bus.")
    except Exception as e:
        logger.warning(f"MCP Bus unavailable: {e}. Running in standalone mode.")
    global ready
    ready = True
    yield
    logger.info("Critic agent is shutting down.")

app = FastAPI(
    title="Critic Agent API",
    description="Content analysis and critique services for editorial quality control",
    version="2.0.0",
    lifespan=lifespan
)

# Initialize metrics
metrics = JustNewsMetrics("critic")

# Register shutdown endpoint if available
try:
    from agents.common.shutdown import register_shutdown_endpoint
    register_shutdown_endpoint(app)
except Exception:
    logger.debug("shutdown endpoint not registered for critic")

# Register reload endpoint if available
try:
    from agents.common.reload import register_reload_endpoint
    register_reload_endpoint(app)
except Exception:
    logger.debug("reload endpoint not registered for critic")

# Add metrics middleware
app.middleware("http")(metrics.request_middleware)

# Pydantic models
class ToolCall(BaseModel):
    """Standard MCP tool call format"""
    args: List[Any]
    kwargs: Dict[str, Any]

class CritiqueRequest(BaseModel):
    """Critique request model"""
    content: str
    url: Optional[str] = None
    context: Optional[str] = None

class ArgumentAnalysisRequest(BaseModel):
    """Argument structure analysis request"""
    text: str
    url: Optional[str] = None

class ConsistencyRequest(BaseModel):
    """Editorial consistency request"""
    text: str
    url: Optional[str] = None

class FallacyRequest(BaseModel):
    """Logical fallacy detection request"""
    text: str
    url: Optional[str] = None

class CredibilityRequest(BaseModel):
    """Source credibility assessment request"""
    text: str
    url: Optional[str] = None

# Health and status endpoints
@app.get("/health")
def health():
    """Health check endpoint"""
    return {"status": "ok", "agent": "critic", "version": "2.0.0"}

@app.get("/ready")
def ready_endpoint():
    """Readiness check endpoint"""
    return {"ready": ready}

@app.get("/metrics")
def get_metrics():
    """Prometheus metrics endpoint"""
    from fastapi.responses import Response
    return Response(metrics.get_metrics(), media_type="text/plain")

# Core critique endpoints
@app.post("/critique_synthesis")
def critique_synthesis_endpoint(call: ToolCall):
    """Synthesize comprehensive content critique"""
    try:
        from .tools import critique_synthesis
        logger.info(f"Calling critique_synthesis with {len(call.args)} args")

        result = critique_synthesis(*call.args, **call.kwargs)

        # Log performance metrics
        if isinstance(result, dict) and "critique_score" in result:
            logger.info(f"‚úÖ Critique synthesis complete: score {result['critique_score']:.1f}/10")

        return result

    except Exception as e:
        logger.error(f"‚ùå Critique synthesis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/critique_neutrality")
def critique_neutrality_endpoint(call: ToolCall):
    """Analyze content neutrality and bias indicators"""
    try:
        from .tools import critique_neutrality
        logger.info(f"Calling critique_neutrality with {len(call.args)} args")

        result = critique_neutrality(*call.args, **call.kwargs)

        # Log performance metrics
        if isinstance(result, dict) and "neutrality_score" in result:
            logger.info(f"‚úÖ Neutrality analysis complete: score {result['neutrality_score']:.1f}/10")

        return result

    except Exception as e:
        logger.error(f"‚ùå Neutrality analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Specialized analysis endpoints
@app.post("/analyze_argument_structure")
def analyze_argument_structure_endpoint(request: ArgumentAnalysisRequest):
    """Analyze argument structure in content"""
    try:
        from .tools import analyze_argument_structure
        logger.info(f"Analyzing argument structure for {len(request.text)} characters")

        result = analyze_argument_structure(request.text, request.url)

        # Log performance metrics
        if isinstance(result, dict) and "argument_strength" in result:
            strength = result["argument_strength"].get("strength_score", 0.0)
            logger.info(f"‚úÖ Argument analysis complete: strength {strength:.2f}")

        return result

    except Exception as e:
        logger.error(f"‚ùå Argument structure analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/assess_editorial_consistency")
def assess_editorial_consistency_endpoint(request: ConsistencyRequest):
    """Assess editorial consistency and coherence"""
    try:
        from .tools import assess_editorial_consistency
        logger.info(f"Assessing editorial consistency for {len(request.text)} characters")

        result = assess_editorial_consistency(request.text, request.url)

        # Log performance metrics
        if isinstance(result, dict) and "coherence_score" in result:
            coherence = result["coherence_score"]
            logger.info(f"‚úÖ Consistency assessment complete: coherence {coherence:.2f}")

        return result

    except Exception as e:
        logger.error(f"‚ùå Editorial consistency error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/detect_logical_fallacies")
def detect_logical_fallacies_endpoint(request: FallacyRequest):
    """Detect logical fallacies in content"""
    try:
        from .tools import detect_logical_fallacies
        logger.info(f"Detecting logical fallacies in {len(request.text)} characters")

        result = detect_logical_fallacies(request.text, request.url)

        # Log performance metrics
        if isinstance(result, dict) and "fallacy_count" in result:
            count = result["fallacy_count"]
            logger.info(f"‚úÖ Fallacy detection complete: {count} fallacies found")

        return result

    except Exception as e:
        logger.error(f"‚ùå Logical fallacy detection error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/assess_source_credibility")
def assess_source_credibility_endpoint(request: CredibilityRequest):
    """Assess source credibility and evidence quality"""
    try:
        from .tools import assess_source_credibility
        logger.info(f"Assessing source credibility for {len(request.text)} characters")

        result = assess_source_credibility(request.text, request.url)

        # Log performance metrics
        if isinstance(result, dict) and "credibility_score" in result:
            credibility = result["credibility_score"]
            logger.info(f"‚úÖ Credibility assessment complete: score {credibility:.2f}")

        return result

    except Exception as e:
        logger.error(f"‚ùå Source credibility error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Utility endpoints
@app.post("/health_check")
def health_check_endpoint(call: ToolCall):
    """Perform comprehensive health check"""
    try:
        from .tools import health_check
        logger.info("Performing critic agent health check")

        result = health_check()

        # Log health status
        if isinstance(result, dict) and "overall_status" in result:
            status = result["overall_status"]
            logger.info(f"üè• Health check: {status}")

        return result

    except Exception as e:
        logger.error(f"‚ùå Health check error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/validate_critique_result")
def validate_critique_result_endpoint(call: ToolCall):
    """Validate critique result structure"""
    try:
        from .tools import validate_critique_result
        logger.info("Validating critique result")

        result = validate_critique_result(*call.args, **call.kwargs)
        return {"valid": result}

    except Exception as e:
        logger.error(f"‚ùå Result validation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/format_critique_output")
def format_critique_output_endpoint(call: ToolCall):
    """Format critique result for output"""
    try:
        from .tools import format_critique_output
        logger.info("Formatting critique output")

        result = format_critique_output(*call.args, **call.kwargs)
        return {"formatted_output": result}

    except Exception as e:
        logger.error(f"‚ùå Output formatting error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/get_critic_engine")
def get_critic_engine_endpoint(call: ToolCall):
    """Get critic engine instance"""
    try:
        from .tools import get_critic_engine
        logger.info("Retrieving critic engine instance")

        engine = get_critic_engine()
        return {"engine_status": "available", "model_count": len(engine.models) if hasattr(engine, 'models') else 0}

    except Exception as e:
        logger.error(f"‚ùå Engine retrieval error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Feedback logging endpoint
@app.post("/log_feedback")
def log_feedback_endpoint(call: ToolCall):
    """Log feedback for critique performance tracking"""
    try:
        feedback_data = {
            "timestamp": datetime.now().isoformat(),
            "agent": "critic",
            "feedback": call.kwargs.get("feedback", {}),
            "operation": call.kwargs.get("operation", "unknown")
        }
        logger.info(f"üìù Feedback logged: {feedback_data['operation']}")
        return feedback_data

    except Exception as e:
        logger.error(f"‚ùå Feedback logging error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn

    host = os.environ.get("CRITIC_HOST", "0.0.0.0")
    port = CRITIC_AGENT_PORT

    logger.info(f"Starting Critic Agent on {host}:{port}")
    uvicorn.run(app, host=host, port=port)