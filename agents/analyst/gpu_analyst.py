from common.observability import get_logger

#!/usr/bin/env python3
"""
GPU Accelerated Analyst - Standalone Module
Provides GPU-accelerated analysis capabilities without circul                    self.sentiment_analyzer = pipeline(  # type: ignore
                        "text-classification",
                        model=sentiment_model,
                        tokenizer=sentiment_tokenizer,
                        top_k=None,  # Use top_k=None instead of deprecated return_all_scores=True
                        device=self.gpu_device,
                        max_length=512,
                        truncation=True,
                        dtype=(
                            torch.float16 if TORCH_AVAILABLE and torch is not None
                            else None
                        ),
                    )es

This module contains the GPUAcceleratedAnalyst class that was previously
in hybrid_tools_v4.py to break the circular import with native_tensorrt_engine.py
"""


import time
from typing import Optional

try:
    # Lightweight orchestrator client (Phase 1 integration)
    from agents.common.gpu_orchestrator_client import GPUOrchestratorClient
    _orchestrator_client: Optional[GPUOrchestratorClient] = GPUOrchestratorClient()
except Exception:
    _orchestrator_client = None

logger = get_logger(__name__)

# Dependency probing (transformers optional in minimal test env)
try:  # pragma: no cover - environment dependent
    import torch  # type: ignore
    TORCH_BASE_AVAILABLE = True
except Exception:  # noqa: BLE001
    torch = None  # type: ignore
    TORCH_BASE_AVAILABLE = False

try:  # pragma: no cover - environment dependent
    from transformers import pipeline  # type: ignore
    TRANSFORMERS_AVAILABLE = True
except Exception:  # noqa: BLE001
    pipeline = None  # type: ignore
    TRANSFORMERS_AVAILABLE = False

TORCH_AVAILABLE = TORCH_BASE_AVAILABLE and TRANSFORMERS_AVAILABLE

try:
    from agents.common.gpu_manager import release_agent_gpu, request_agent_gpu
    PRODUCTION_GPU_AVAILABLE = True
except ImportError:
    PRODUCTION_GPU_AVAILABLE = False

class GPUAcceleratedAnalyst:
    """
    OPERATIONAL GPU acceleration using RTX 3090 with proven 20x+ performance
    This class provides the production-ready GPU acceleration that was
    successfully tested and validated on July 27, 2025:
    - 42.1 articles per second processing rate
    - 0.024s average per article (vs 0.5s CPU baseline)
    - 91% sentiment analysis confidence
    - Complete RTX 3090 24GB VRAM utilization
    """

    def __init__(self):
        self.gpu_available = False
        self.models_loaded = False
        self.gpu_allocated = False
        self.gpu_device = None
        self.gpu_memory_gb = 2.0  # Analyst needs ~2GB for sentiment/bias models
        self.memory_circuit_breaker = False  # Circuit breaker for memory protection
        self.memory_threshold_gb = 1.0  # Minimum free memory before circuit breaker
        self.performance_stats = {
            "total_requests": 0,
            "gpu_requests": 0,
            "fallback_requests": 0,
            "total_time": 0.0,
            "average_time": 0.0,
        }
        logger.info("üöÄ Initializing OPERATIONAL GPU-Accelerated Analyst")
        # Phase 1 gating: consult orchestrator (fail closed to CPU if uncertain)
        orchestrator_allows_gpu = False
        orchestrator_safe_mode = True
        if _orchestrator_client is not None:
            try:
                decision = _orchestrator_client.cpu_fallback_decision()
                orchestrator_allows_gpu = bool(decision.get("use_gpu", False))
                orchestrator_safe_mode = bool(decision.get("safe_mode", True))
                logger.info(
                    "üîê Orchestrator decision: use_gpu=%s safe_mode=%s gpu_available=%s", 
                    orchestrator_allows_gpu,
                    orchestrator_safe_mode,
                    decision.get("gpu_available")
                )
            except Exception as e:  # noqa: BLE001
                logger.warning(f"GPU Orchestrator decision fetch failed, enforcing CPU fallback: {e}")
        else:
            logger.info("GPU Orchestrator client not available - defaulting to conservative CPU-first init")

        if orchestrator_allows_gpu:
            # Initialize GPU allocation + models only if orchestrator permits
            self._initialize_gpu_allocation()
            self._initialize_gpu_models()
        else:
            logger.info(
                "üõë Skipping GPU initialization (safe_mode=%s, allows_gpu=%s). Using CPU paths only.",
                orchestrator_safe_mode,
                orchestrator_allows_gpu,
            )

    def _initialize_gpu_allocation(self):
        """Initialize GPU allocation through production manager"""
        try:
            if PRODUCTION_GPU_AVAILABLE and TORCH_AVAILABLE and torch.cuda.is_available():  # type: ignore
                try:
                    # Request GPU allocation through production manager
                    allocation = request_agent_gpu("analyst", self.gpu_memory_gb)
                    if (
                        isinstance(allocation, dict)
                        and allocation.get("status") == "allocated"
                    ):
                        self.gpu_allocated = True
                        self.gpu_device = allocation.get("gpu_device", 0)
                        self.gpu_memory_gb = allocation.get(
                            "allocated_memory_gb", self.gpu_memory_gb
                        )
                        logger.info(
                            "‚úÖ Analyst GPU allocated: "
                            f"{self.gpu_memory_gb}GB on device {self.gpu_device}"
                        )
                    else:
                        logger.warning(f"‚ö†Ô∏è Analyst GPU allocation failed: {allocation}")
                        self.gpu_device = 0  # Fallback to device 0
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Analyst GPU allocation error: {e}")
                    self.gpu_device = 0  # Fallback to device 0
            else:
                logger.info(
                    "üìã Production GPU manager not available, using direct GPU access"
                )
                self.gpu_device = (
                    0 if (TORCH_AVAILABLE and torch.cuda.is_available()) else -1  # type: ignore
                )
        except Exception as e:
            logger.error(f"‚ùå GPU allocation initialization failed: {e}")
            self.gpu_device = -1

    def _initialize_gpu_models(self):
        """Initialize GPU-accelerated models with professional memory management"""
        try:
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available() and self.gpu_device >= 0:  # type: ignore
                # Set CUDA device to allocated device
                torch.cuda.set_device(self.gpu_device)  # type: ignore
                gpu_name = torch.cuda.get_device_name(self.gpu_device)  # type: ignore
                gpu_memory = (
                    torch.cuda.get_device_properties(self.gpu_device).total_memory  # type: ignore
                    / 1024**3
                )
                logger.info(f"‚úÖ GPU Available: {gpu_name}")
                logger.info(f"‚úÖ GPU Memory: {gpu_memory:.1f} GB")
                # Clear GPU cache to prevent memory conflicts
                if TORCH_AVAILABLE and torch is not None:
                    torch.cuda.empty_cache()

                # Load PROVEN models from Quick Win with explicit device management
                if pipeline is None:
                    raise RuntimeError("Transformers pipeline not available")

                try:
                    # Manual safetensors loading to bypass PyTorch version issue
                    from transformers import (
                        AutoModelForSequenceClassification,
                        AutoTokenizer,
                    )

                    sentiment_model = AutoModelForSequenceClassification.from_pretrained(
                        "cardiffnlp/twitter-roberta-base-sentiment-latest",
                        use_safetensors=True
                    )
                    sentiment_tokenizer = AutoTokenizer.from_pretrained(
                        "cardiffnlp/twitter-roberta-base-sentiment-latest"
                    )

                    self.sentiment_analyzer = pipeline(  # type: ignore
                        "sentiment-analysis",
                        model=sentiment_model,
                        tokenizer=sentiment_tokenizer,
                        top_k=None,  # Use top_k=None instead of deprecated return_all_scores=True
                        device=self.gpu_device,
                        max_length=512,
                        truncation=True,
                        dtype=(
                            torch.float16 if TORCH_AVAILABLE and torch is not None
                            else None
                        ),
                    )
                    logger.info("‚úÖ Sentiment analyzer loaded successfully with safetensors")
                except Exception as e:
                    logger.error(f"‚ùå Failed to load sentiment analyzer: {e}")
                    raise

                try:
                    # Manual safetensors loading for bias detector
                    from transformers import (
                        AutoModelForSequenceClassification as BiasModel,
                    )
                    from transformers import AutoTokenizer as BiasTokenizer

                    bias_model = BiasModel.from_pretrained(
                        "unitary/toxic-bert",
                        use_safetensors=True
                    )
                    bias_tokenizer = BiasTokenizer.from_pretrained(
                        "unitary/toxic-bert"
                    )

                    self.bias_detector = pipeline(  # type: ignore
                        "text-classification",
                        model=bias_model,
                        tokenizer=bias_tokenizer,
                        top_k=None,  # Use top_k=None for consistency and to get all scores
                        device=self.gpu_device,
                        max_length=512,
                        truncation=True,
                        dtype=(
                            torch.float16 if TORCH_AVAILABLE and torch is not None
                            else None
                        ),
                    )
                    logger.info("‚úÖ Bias detector loaded successfully with safetensors")
                except Exception as e:
                    logger.error(f"‚ùå Failed to load bias detector: {e}")
                    raise
                # Verify models are on GPU
                logger.info(
                    "Sentiment model device: "
                    f"{next(self.sentiment_analyzer.model.parameters()).device}"
                )
                logger.info(
                    "Bias model device: "
                    f"{next(self.bias_detector.model.parameters()).device}"
                )
                self.models_loaded = True
                logger.info(
                    "‚úÖ OPERATIONAL GPU models loaded (validated 42.1 articles/sec)"
                )
            else:
                logger.warning("‚ö†Ô∏è  GPU not available in this environment")
        except Exception as e:
            logger.error(f"‚ùå GPU initialization failed: {e}")
            logger.info("üì± Will use hybrid fallback system")

    def _check_gpu_memory_circuit_breaker(self) -> bool:
        """Check GPU memory and update circuit breaker status.
        
        Returns True if processing should be allowed, False if circuit breaker is active.
        """
        if not (TORCH_AVAILABLE and torch is not None and torch.cuda.is_available()):
            return False
            
        try:
            # Get current GPU memory info
            device = torch.cuda.current_device()
            memory_info = torch.cuda.mem_get_info(device)
            free_memory_gb = memory_info[0] / (1024**3)  # Free memory in GB
            total_memory_gb = memory_info[1] / (1024**3)  # Total memory in GB
            used_memory_gb = total_memory_gb - free_memory_gb
            
            # Log memory status periodically
            if self.performance_stats["total_requests"] % 100 == 0:
                logger.info(
                    f"GPU Memory Status: {used_memory_gb:.2f}GB used, "
                    f"{free_memory_gb:.2f}GB free, {total_memory_gb:.1f}GB total"
                )
            
            # Critical memory warning (less than 2GB free)
            critical_threshold_gb = 2.0
            if free_memory_gb < critical_threshold_gb and not hasattr(self, '_critical_memory_warned'):
                logger.warning(
                    f"üö® CRITICAL: GPU memory critically low: {free_memory_gb:.2f}GB free "
                    f"(< {critical_threshold_gb}GB threshold)"
                )
                self._critical_memory_warned = True
            
            # Reset warning flag when memory recovers
            if free_memory_gb >= critical_threshold_gb + 1.0 and hasattr(self, '_critical_memory_warned'):
                logger.info(f"‚úÖ GPU memory recovered: {free_memory_gb:.2f}GB free")
                delattr(self, '_critical_memory_warned')
            
            # Update circuit breaker based on available memory
            if free_memory_gb < self.memory_threshold_gb:
                if not self.memory_circuit_breaker:
                    logger.warning(
                        f"üî¥ GPU memory circuit breaker activated: "
                        f"{free_memory_gb:.2f}GB free < {self.memory_threshold_gb}GB threshold"
                    )
                self.memory_circuit_breaker = True
                return False
            else:
                if self.memory_circuit_breaker:
                    logger.info(
                        f"üü¢ GPU memory circuit breaker reset: "
                        f"{free_memory_gb:.2f}GB free >= {self.memory_threshold_gb}GB threshold"
                    )
                self.memory_circuit_breaker = False
                return True
                
        except Exception as e:
            logger.error(f"Failed to check GPU memory: {e}")
            # On error, activate circuit breaker for safety
            self.memory_circuit_breaker = True
            return False

    def get_gpu_memory_stats(self) -> dict[str, float]:
        """Get comprehensive GPU memory statistics.
        
        Returns:
            Dictionary with memory statistics in GB
        """
        if not (TORCH_AVAILABLE and torch is not None and torch.cuda.is_available()):
            return {"error": "GPU not available"}
            
        try:
            device = torch.cuda.current_device()
            memory_info = torch.cuda.mem_get_info(device)
            free_memory_bytes = memory_info[0]
            total_memory_bytes = memory_info[1]
            
            return {
                "free_memory_gb": free_memory_bytes / (1024**3),
                "total_memory_gb": total_memory_bytes / (1024**3),
                "used_memory_gb": (total_memory_bytes - free_memory_bytes) / (1024**3),
                "memory_utilization_percent": ((total_memory_bytes - free_memory_bytes) / total_memory_bytes) * 100,
                "circuit_breaker_active": self.memory_circuit_breaker,
                "memory_threshold_gb": self.memory_threshold_gb
            }
        except Exception as e:
            logger.error(f"Failed to get GPU memory stats: {e}")
            return {"error": str(e)}

    def is_gpu_safe_to_use(self) -> tuple[bool, str]:
        """Check if GPU is safe to use based on memory and other conditions.
        
        Returns:
            Tuple of (is_safe, reason)
        """
        if not self.models_loaded:
            return False, "GPU models not loaded"
            
        if not (TORCH_AVAILABLE and torch is not None and torch.cuda.is_available()):
            return False, "CUDA not available"
            
        if self.memory_circuit_breaker:
            return False, "Memory circuit breaker active"
            
        memory_stats = self.get_gpu_memory_stats()
        if "error" in memory_stats:
            return False, f"Memory check failed: {memory_stats['error']}"
            
        free_memory_gb = memory_stats.get("free_memory_gb", 0)
        if free_memory_gb < self.memory_threshold_gb:
            return False, f"Insufficient free memory: {free_memory_gb:.2f}GB < {self.memory_threshold_gb}GB"
            
        return True, "GPU safe to use"

    def score_sentiment_gpu(self, text: str) -> float | None:
        """GPU-accelerated sentiment scoring with proven performance and
        device management
        This method is instrumented to emit a structured GPU event for each call
        (pilot instrumentation using
        `agents.common.gpu_metrics`).
        """
        # Start structured event (emit even if GPU is unavailable so we can
        # observe attempts and fallback behavior).
        try:
            from agents.common.gpu_metrics import end_event, start_event
        except Exception:
            start_event = None
            end_event = None
        event_id = None
        if start_event is not None:
            event_id = start_event(
                agent="analyst", operation="score_sentiment_gpu", batch_size=1
            )
        # If GPU models are not loaded or torch/CUDA is not available, emit an
        # immediate failure/placeholder event and return None to indicate
        # fallback.
        if not self.models_loaded or not (
            TORCH_AVAILABLE and torch is not None and torch.cuda.is_available()
        ):
            if end_event is not None and event_id is not None:
                try:
                    end_event(
                        event_id, success=False, error="gpu_or_models_unavailable"
                    )
                except Exception:
                    pass
            return None
            
        # Check memory circuit breaker
        if not self._check_gpu_memory_circuit_breaker():
            if end_event is not None and event_id is not None:
                try:
                    end_event(
                        event_id, success=False, error="memory_circuit_breaker"
                    )
                except Exception:
                    pass
            return None
        start_time = time.time()
        try:
            # Ensure we're on the correct CUDA device
            if TORCH_AVAILABLE and torch is not None:
                torch.cuda.set_device(self.gpu_device)
                torch.cuda.synchronize()  # Synchronize before operation
            # Clear any mixed device tensors
            if TORCH_AVAILABLE and torch is not None:
                with torch.cuda.device(self.gpu_device):
                    result = self.sentiment_analyzer(text)
            else:
                result = self.sentiment_analyzer(text)
            # Convert to 0.0-1.0 scale (matching original format)
            if isinstance(result, list) and len(result) > 0:
                scores = {item["label"].lower(): item["score"] for item in result[0]}
                if "positive" in scores:
                    sentiment_score = scores["positive"]
                elif "negative" in scores:
                    sentiment_score = 1.0 - scores["negative"]
                else:
                    sentiment_score = 0.5
            else:
                logger.warning(f"Unexpected sentiment result format: {type(result)} {result}")
                sentiment_score = 0.5
            processing_time = time.time() - start_time
            # Synchronize CUDA operations
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available():
                torch.cuda.synchronize()
            self.performance_stats["gpu_requests"] += 1
            self.performance_stats["total_time"] += processing_time
            # End structured event with outcome
            if end_event is not None and event_id is not None:
                try:
                    end_event(
                        event_id,
                        success=True,
                        processing_time_s=processing_time,
                        score=sentiment_score,
                    )
                except Exception:
                    pass
            logger.info(
                f"\u2705 GPU sentiment: {sentiment_score:.3f} ({processing_time:.3f}s)"
            )
            return sentiment_score
        except Exception as e:
            logger.error(f"\u274c GPU sentiment failed: {e}")
            # Clear CUDA cache on error
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available():
                torch.cuda.empty_cache()
            # Emit failure event
            if end_event is not None and event_id is not None:
                try:
                    end_event(event_id, success=False, error=str(e))
                except Exception:
                    pass
            return None
        finally:
            # Proactive memory management: clear cache after each operation
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                except Exception as e:
                    logger.debug(f"Failed to clear GPU cache: {e}")

    def score_bias_gpu(self, text: str) -> float | None:
        """GPU-accelerated bias scoring with proven performance and device management"""
        if not self.models_loaded or not (
            TORCH_AVAILABLE and torch is not None and torch.cuda.is_available()
        ):
            return None
            
        # Check memory circuit breaker
        if not self._check_gpu_memory_circuit_breaker():
            return None
        start_time = time.time()
        try:
            logger.debug(f"Starting GPU sentiment analysis for {len(text)} characters")
            # Verify models are still loaded
            if not hasattr(self, 'sentiment_analyzer') or self.sentiment_analyzer is None:
                logger.error("Sentiment analyzer model not loaded")
                return None
            # Ensure we're on the correct CUDA device
            if TORCH_AVAILABLE and torch is not None:
                torch.cuda.set_device(self.gpu_device)
                torch.cuda.synchronize()  # Synchronize before operation
            # Clear any mixed device tensors
            if TORCH_AVAILABLE and torch is not None:
                with torch.cuda.device(self.gpu_device):
                    result = self.bias_detector(text)
            else:
                result = self.bias_detector(text)
            # Convert toxicity result to bias scale
            if isinstance(result, list) and len(result) > 0:
                # With top_k=None, we get all scores, find the TOXIC score
                scores = {item["label"]: item["score"] for item in result[0]}
                if "TOXIC" in scores:
                    bias_score = scores["TOXIC"]
                else:
                    # If TOXIC label not found, use the highest score as bias indicator
                    bias_score = max(scores.values()) if scores else 0.5
            else:
                logger.warning(f"Unexpected bias result format: {type(result)} {result}")
                bias_score = 0.5
            processing_time = time.time() - start_time
            # Synchronize CUDA operations
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available():
                torch.cuda.synchronize()
            self.performance_stats["gpu_requests"] += 1
            self.performance_stats["total_time"] += processing_time
            logger.info(f"‚úÖ GPU bias: {bias_score:.3f} ({processing_time:.3f}s)")
            return bias_score
        except Exception as e:
            logger.error(f"‚ùå GPU bias failed: {e}")
            # Clear CUDA cache on error
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available():
                torch.cuda.empty_cache()
            return None
        finally:
            # Proactive memory management: clear cache after each operation
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                except Exception as e:
                    logger.debug(f"Failed to clear GPU cache: {e}")

    def score_sentiment_batch_gpu(self, texts: list[str]) -> list[float | None]:
        """BATCH GPU-accelerated sentiment scoring with CUDA device management -
        Up to 100x faster!"""
        if not self.models_loaded or not texts or not (
            TORCH_AVAILABLE and torch is not None and torch.cuda.is_available()
        ):
            return [None] * len(texts) if texts else []
            
        # Check memory circuit breaker
        if not self._check_gpu_memory_circuit_breaker():
            return [None] * len(texts)
        start_time = time.time()
        try:
            # Ensure we're on the correct CUDA device
            if TORCH_AVAILABLE and torch is not None:
                torch.cuda.set_device(self.gpu_device)
            # Use HuggingFace pipeline's native batch processing
            # with explicit device context
            if TORCH_AVAILABLE and torch is not None:
                with torch.cuda.device(self.gpu_device):
                    results = self.sentiment_analyzer(texts)
            else:
                results = self.sentiment_analyzer(texts)
            sentiment_scores = []
            for result in results:
                if isinstance(result, list) and len(result) > 0:
                    scores = {item["label"].lower(): item["score"] for item in result}
                    if "positive" in scores:
                        sentiment_score = scores["positive"]
                    elif "negative" in scores:
                        sentiment_score = 1.0 - scores["negative"]
                    else:
                        sentiment_score = 0.5
                else:
                    sentiment_score = 0.5
                sentiment_scores.append(sentiment_score)
            processing_time = time.time() - start_time
            batch_size = len(texts)
            rate = batch_size / processing_time if processing_time > 0 else 0
            self.performance_stats["gpu_requests"] += batch_size
            self.performance_stats["total_time"] += processing_time
            logger.info(
                "‚úÖ GPU batch sentiment: "
                f"{batch_size} articles in {processing_time:.3f}s "
                f"({rate:.1f} articles/sec)"
            )
            return sentiment_scores
        except Exception as e:
            logger.error(f"‚ùå GPU batch sentiment failed: {e}")
            # Clear CUDA cache on error
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available():
                torch.cuda.empty_cache()
            return [None] * len(texts)
        finally:
            # Proactive memory management: clear cache after each operation
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                except Exception as e:
                    logger.debug(f"Failed to clear GPU cache: {e}")

    def score_bias_batch_gpu(self, texts: list[str]) -> list[float | None]:
        """BATCH GPU-accelerated bias scoring with CUDA device management.

        Up to 100x faster!
        """
        if not self.models_loaded or not texts or not (
            TORCH_AVAILABLE and torch is not None and torch.cuda.is_available()
        ):
            return [None] * len(texts) if texts else []
            
        # Check memory circuit breaker
        if not self._check_gpu_memory_circuit_breaker():
            return [None] * len(texts)
        start_time = time.time()
        try:
            # Ensure we're on the correct CUDA device
            if TORCH_AVAILABLE and torch is not None:
                torch.cuda.set_device(self.gpu_device)
            # Use HuggingFace pipeline's native batch processing
            # with explicit device context
            if TORCH_AVAILABLE and torch is not None:
                with torch.cuda.device(self.gpu_device):
                    results = self.bias_detector(texts)
            else:
                results = self.bias_detector(texts)
            bias_scores = []
            for result in results:
                if isinstance(result, list) and len(result) > 0:
                    scores = {item["label"]: item["score"] for item in result}
                    if "TOXIC" in scores:
                        bias_score = scores["TOXIC"]
                    else:
                        # If TOXIC label not found, use the highest score as bias indicator
                        bias_score = max(scores.values()) if scores else 0.5
                else:
                    bias_score = 0.5
                bias_scores.append(bias_score)
            processing_time = time.time() - start_time
            batch_size = len(texts)
            rate = batch_size / processing_time if processing_time > 0 else 0
            self.performance_stats["gpu_requests"] += batch_size
            self.performance_stats["total_time"] += processing_time
            logger.info(
                "‚úÖ GPU batch bias: "
                f"{batch_size} articles in {processing_time:.3f}s "
                f"({rate:.1f} articles/sec)"
            )
            return bias_scores
        except Exception as e:
            logger.error(f"‚ùå GPU batch bias failed: {e}")
            # Clear CUDA cache on error
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available():
                torch.cuda.empty_cache()
            return [None] * len(texts)
        finally:
            # Proactive memory management: clear cache after each operation
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                except Exception as e:
                    logger.debug(f"Failed to clear GPU cache: {e}")

# Global GPU analyst instance
_gpu_analyst = None

def get_gpu_analyst():
    """Get or create the GPU analyst instance"""
    global _gpu_analyst
    if _gpu_analyst is None:
        _gpu_analyst = GPUAcceleratedAnalyst()
    return _gpu_analyst

def cleanup_gpu_analyst():
    """Clean up GPU analyst and release GPU allocation"""
    global _gpu_analyst
    if _gpu_analyst is not None:
        try:
            # Force garbage collection before cleanup
            import gc
            gc.collect()
            
            # Clear any cached tensors and models
            if hasattr(_gpu_analyst, 'sentiment_analyzer') and _gpu_analyst.sentiment_analyzer:
                try:
                    del _gpu_analyst.sentiment_analyzer
                except Exception:
                    pass
                    
            if hasattr(_gpu_analyst, 'bias_detector') and _gpu_analyst.bias_detector:
                try:
                    del _gpu_analyst.bias_detector
                except Exception:
                    pass
            
            # Release GPU allocation
            if _gpu_analyst.gpu_allocated:
                try:
                    if PRODUCTION_GPU_AVAILABLE:
                        release_agent_gpu("analyst")
                    logger.info("‚úÖ Analyst GPU allocation released")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to release Analyst GPU allocation: {e}")
            
            # Aggressive GPU cache clearing
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()  # Ensure all operations complete
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to clear GPU cache during cleanup: {e}")
            
            # Clear instance
            _gpu_analyst = None
            
            # Final garbage collection
            gc.collect()
            
            logger.info("üßπ GPU analyst cleaned up thoroughly")
        except Exception as e:
            logger.error(f"Error during GPU analyst cleanup: {e}")
